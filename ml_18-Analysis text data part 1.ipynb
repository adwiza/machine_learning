{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5841b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трисия\n",
      "любила\n",
      "Нью\n",
      "-\n",
      "Йорк\n",
      ",\n",
      "поскольку\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "s = u'Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.'\n",
    "\n",
    "for t in tokenizer.tokenize(s)[:7]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09c8d1",
   "metadata": {},
   "source": [
    "# Библиотека FTFY: fixes text for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01d6e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm blue, da ba dee da ba doo...\n"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_text\n",
    "\n",
    "print(fix_text(u'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m', normalization='NFKC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49697b28",
   "metadata": {},
   "source": [
    "# Стоп слова\n",
    "Наиболее частые слова в языке, не содержащие никакой информации о содержании текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e6f101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и в во не что он на я с со как а то все она так его но да ты\n",
      "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# print(nltk.data.path.pop(-2))\n",
    "nltk.data.path.append('D:\\ML/nltk_data')\n",
    "print(' '.join(stopwords.words('russian')[:20]))\n",
    "print(' '.join(stopwords.words('english')[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecb201",
   "metadata": {},
   "source": [
    "# Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcea020",
   "metadata": {},
   "source": [
    "Приведение токенов к единому виду, чтобы избавиться от поверхностной разницы в написании\n",
    "\n",
    "Подходы\n",
    "* сформулировать набор правилд, по которым преобразуется токен <br>\n",
    "Нью-Йорк->нью->йорк->ньюиорк\n",
    "* явно хранить связи между токенами (Wordnet-Princeton)<br>\n",
    "машина -> автомобиль, Windows 6 -> window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0231df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = 'Нью-Йорк'\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a8da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюйорк\n"
     ]
    }
   ],
   "source": [
    "# s1 = s1.replace('-', '')\n",
    "# print(s1)\n",
    "import re\n",
    "s2 = re.sub('\\W', '', s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14caaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюиорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub('й', u'и', s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855f05f",
   "metadata": {},
   "source": [
    "# Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd9f4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "stem\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "s = PorterStemmer()\n",
    "print(s.stem('Tokenization'))\n",
    "print(s.stem('stemming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a3d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "r = RussianStemmer()\n",
    "print(r.stem('авиация'))\n",
    "print(r.stem('национальный'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b1710",
   "metadata": {},
   "source": [
    "# Лемматизация\n",
    "(обычно лучше работает для сложных языков, в том числе для русского)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73642dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67fbfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='замок', score=0.428571, methods_stack=((DictionaryAnalyzer(), 'замок', 141, 0),))\n",
      "Word: замок | Normal form замок\n",
      "\n",
      "\n",
      "Metadata: Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='замок', score=0.428571, methods_stack=((DictionaryAnalyzer(), 'замок', 141, 3),))\n",
      "Word: замок | Normal form замок\n",
      "\n",
      "\n",
      "Metadata: Parse(word='замок', tag=OpencorporaTag('VERB,perf,intr masc,sing,past,indc'), normal_form='замокнуть', score=0.142857, methods_stack=((DictionaryAnalyzer(), 'замок', 764, 1),))\n",
      "Word: замок | Normal form замокнуть\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for i in morph.parse('замок'):\n",
    "    print(f'Metadata: {i}')\n",
    "    print(f'Word: {i.word} | Normal form {i.normal_form}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8080dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d927377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1.],\n",
       "       [0., 1., 3.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "text_dict = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = dvectorizer.fit_transform(text_dict)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b56ef9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a3a863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376c3d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'thank': 1, '40': 1, ',': 1, 'mr': 1, 'president': 1, '.': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 3,\n",
       "          'agree': 1,\n",
       "          'recognise': 1,\n",
       "          'turkey': 2,\n",
       "          \"'\": 1,\n",
       "          'european': 1,\n",
       "          'prospects': 2,\n",
       "          'auspicious': 1,\n",
       "          'outcome': 1,\n",
       "          'needs': 1,\n",
       "          ':': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 2,\n",
       "          'firstly': 1,\n",
       "          'would': 1,\n",
       "          'like': 1,\n",
       "          'express': 1,\n",
       "          'sincerest': 1,\n",
       "          'thanks': 1,\n",
       "          'high': 1,\n",
       "          'representative': 1,\n",
       "          'including': 1,\n",
       "          'important': 1,\n",
       "          'issue': 1,\n",
       "          'agenda': 1,\n",
       "          'early': 1,\n",
       "          'stage': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "docs = [\n",
    "    \"Thank 40 you, Mr President.\",\n",
    "    \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
    "    \"Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.\",\n",
    "]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "stopwords_eng = stopwords.words()\n",
    "\n",
    "document_bags = list()\n",
    "\n",
    "for d in docs:\n",
    "    bag = Counter()\n",
    "    text = d.lower()\n",
    "    for t in tokenizer.tokenize(text):\n",
    "        if t in stopwords_eng:\n",
    "            continue\n",
    "        bag[t] += 1\n",
    "    document_bags.append(bag)\n",
    "\n",
    "document_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62411796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 3., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 2., 1., 0., 0., 0., 0., 0., 2., 0.],\n",
       "       [0., 2., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.fit_transform(document_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59221270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 31)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.fit_transform(document_bags).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee984d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " '.',\n",
       " '40',\n",
       " ':',\n",
       " 'agenda',\n",
       " 'agree',\n",
       " 'auspicious',\n",
       " 'early',\n",
       " 'european',\n",
       " 'express',\n",
       " 'firstly',\n",
       " 'high',\n",
       " 'important',\n",
       " 'including',\n",
       " 'issue',\n",
       " 'like',\n",
       " 'madam',\n",
       " 'mr',\n",
       " 'needs',\n",
       " 'outcome',\n",
       " 'president',\n",
       " 'prospects',\n",
       " 'recognise',\n",
       " 'representative',\n",
       " 'sincerest',\n",
       " 'stage',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'turkey',\n",
       " 'would']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvectorizer.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "317115ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>40</th>\n",
       "      <th>:</th>\n",
       "      <th>agenda</th>\n",
       "      <th>agree</th>\n",
       "      <th>auspicious</th>\n",
       "      <th>early</th>\n",
       "      <th>european</th>\n",
       "      <th>...</th>\n",
       "      <th>president</th>\n",
       "      <th>prospects</th>\n",
       "      <th>recognise</th>\n",
       "      <th>representative</th>\n",
       "      <th>sincerest</th>\n",
       "      <th>stage</th>\n",
       "      <th>thank</th>\n",
       "      <th>thanks</th>\n",
       "      <th>turkey</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     '    ,    .   40    :  agenda  agree  auspicious  early  european  ...  \\\n",
       "0  0.0  1.0  1.0  1.0  0.0     0.0    0.0         0.0    0.0       0.0  ...   \n",
       "1  1.0  3.0  0.0  0.0  1.0     0.0    1.0         1.0    0.0       1.0  ...   \n",
       "2  0.0  2.0  1.0  0.0  0.0     1.0    0.0         0.0    1.0       0.0  ...   \n",
       "\n",
       "   president  prospects  recognise  representative  sincerest  stage  thank  \\\n",
       "0        1.0        0.0        0.0             0.0        0.0    0.0    1.0   \n",
       "1        1.0        2.0        1.0             0.0        0.0    0.0    0.0   \n",
       "2        1.0        0.0        0.0             1.0        1.0    1.0    0.0   \n",
       "\n",
       "   thanks  turkey  would  \n",
       "0     0.0     0.0    0.0  \n",
       "1     0.0     2.0    0.0  \n",
       "2     1.0     0.0    1.0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dvectorizer.fit_transform(document_bags), columns=dvectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475c8c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0,\n",
       "         0, 0, 0, 0, 2],\n",
       "        [0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sklearn_vectorizer = CountVectorizer(stop_words='english')\n",
    "sklearn_vectorizer.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6355406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 23,\n",
       " '40': 0,\n",
       " 'mr': 14,\n",
       " 'president': 17,\n",
       " 'madam': 13,\n",
       " 'agree': 2,\n",
       " 'recognise': 19,\n",
       " 'turkey': 25,\n",
       " 'european': 5,\n",
       " 'prospects': 18,\n",
       " 'auspicious': 3,\n",
       " 'outcome': 16,\n",
       " 'needs': 15,\n",
       " 'firstly': 7,\n",
       " 'like': 12,\n",
       " 'express': 6,\n",
       " 'sincerest': 21,\n",
       " 'thanks': 24,\n",
       " 'high': 8,\n",
       " 'representative': 20,\n",
       " 'including': 10,\n",
       " 'important': 9,\n",
       " 'issue': 11,\n",
       " 'agenda': 1,\n",
       " 'early': 4,\n",
       " 'stage': 22}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdbbde",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee4ad560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.54645401, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.54645401,\n",
       "         0.        , 0.        , 0.32274454, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.54645401, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.25882751, 0.25882751, 0.        ,\n",
       "         0.25882751, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.19684499, 0.        ,\n",
       "         0.25882751, 0.25882751, 0.1528677 , 0.51765502, 0.25882751,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.51765502],\n",
       "        [0.        , 0.26795858, 0.        , 0.        , 0.26795858,\n",
       "         0.        , 0.26795858, 0.26795858, 0.26795858, 0.26795858,\n",
       "         0.26795858, 0.26795858, 0.26795858, 0.20378941, 0.        ,\n",
       "         0.        , 0.        , 0.15826066, 0.        , 0.        ,\n",
       "         0.26795858, 0.26795858, 0.26795858, 0.        , 0.26795858,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "features = vectorizer.fit_transform(docs).todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5207a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 23,\n",
       " '40': 0,\n",
       " 'mr': 14,\n",
       " 'president': 17,\n",
       " 'madam': 13,\n",
       " 'agree': 2,\n",
       " 'recognise': 19,\n",
       " 'turkey': 25,\n",
       " 'european': 5,\n",
       " 'prospects': 18,\n",
       " 'auspicious': 3,\n",
       " 'outcome': 16,\n",
       " 'needs': 15,\n",
       " 'firstly': 7,\n",
       " 'like': 12,\n",
       " 'express': 6,\n",
       " 'sincerest': 21,\n",
       " 'thanks': 24,\n",
       " 'high': 8,\n",
       " 'representative': 20,\n",
       " 'including': 10,\n",
       " 'important': 9,\n",
       " 'issue': 11,\n",
       " 'agenda': 1,\n",
       " 'early': 4,\n",
       " 'stage': 22}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "906a6d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.54645401, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.54645401,\n",
       "         0.        , 0.        , 0.32274454, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.54645401, 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec29f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
